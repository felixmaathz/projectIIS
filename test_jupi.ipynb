{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from furhat_remote_api import FurhatRemoteAPI\n",
    "import speech_recognition as sr\n",
    "import cv2\n",
    "import opencv_jupyter_ui as jcv2\n",
    "from feat.utils import FEAT_EMOTION_COLUMNS\n",
    "from feat import Detector\n",
    "from train_model import train_model\n",
    "\n",
    "# Detector choice\n",
    "model = train_model()\n",
    "detector = Detector(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabbe\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected emotion: neutral\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce81905df109447e9017843877f73de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='danger', description='Stop', style=ButtonStyle()), HBox(children=(Label(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2feab1444d41d4b986283aa69d5502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='danger', description='Stop', style=ButtonStyle()), HBox(children=(Label(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cde2982aeb44a28a491254a9f018ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>Emotion Detection</center>'), Canvas()), layout=Layout(border_bottom='1.5px…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Exection is stopped\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> 007182882facbb7c246a996952c5b04c58f8e5ab
   "source": [
    "# Furhat IP address\n",
    "FURHAT_IP = \"127.0.1.1\"\n",
    "\n",
    "# Connect to Furhat\n",
    "furhat = FurhatRemoteAPI(FURHAT_IP)\n",
    "furhat.set_led(red=100, green=50, blue=50)\n",
    "\n",
    "# Furhat faces and voices\n",
    "FACES = {'Bartender': 'Brooklyn'}\n",
    "VOICES_EN = {'Bartender': 'GregoryNeural'}\n",
    "\n",
    "emotion_detection_active = False\n",
    "\n",
    "# Function to start emotion detection\n",
    "def start_emotion_detection():\n",
    "    global emotion_detection_active\n",
    "    emotion_detection_active = True\n",
    "\n",
    "# Function to stop emotion detection\n",
    "def stop_emotion_detection():\n",
    "    global emotion_detection_active\n",
    "    emotion_detection_active = False\n",
    "\n",
    "# Furhat speech\n",
    "def bsay(line):\n",
    "    furhat.say(text=line, blocking=True)\n",
    "    sleep(1) \n",
    "\n",
    "# Speech recognition setup\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Function to recognize speech\n",
    "def recognize_speech():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Say something!\")\n",
    "        audio = recognizer.listen(source, timeout=5)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(\"You said:\", text)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech Recognition could not understand audio.\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Function for emotion detection\n",
    "def detect_emotion(frame):\n",
    "    faces = detector.detect_faces(frame)\n",
    "    landmarks = detector.detect_landmarks(frame, faces)\n",
    "    emotions = detector.detect_emotions(frame, faces, landmarks)\n",
    "\n",
    "    faces = faces[0]\n",
    "    emotions = emotions[0]\n",
    "\n",
    "    strongest_emotion = emotions.argmax(axis=1)\n",
    "\n",
    "    for (face, top_emo) in zip(faces, strongest_emotion):\n",
    "        (x0, y0, x1, y1, p) = face\n",
    "        cv2.rectangle(frame, (int(x0), int(y0)), (int(x1), int(y1)), (255, 0, 0), 3)\n",
    "        cv2.putText(frame, FEAT_EMOTION_COLUMNS[top_emo], (int(x0), int(y0 - 10)),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 1.5, (255, 0, 0), 2)\n",
    "        \n",
    "        print(f\"Detected emotion: {FEAT_EMOTION_COLUMNS[top_emo]}\")\n",
    "        # Return the detected emotion\n",
    "        return FEAT_EMOTION_COLUMNS[top_emo] \n",
    "\n",
    "\n",
    "# Function to react to speech\n",
    "def speech(text):\n",
    "    if text:\n",
    "        if \"hello\" in text.lower():\n",
    "            bsay(\"very tasty\")\n",
    "            furhat.gesture(name='Surprise')\n",
    "\n",
    "# Function to react to emotion\n",
    "def emotion(detected_emotion):\n",
    "            print(f\"Detected emotion: {detected_emotion}\")\n",
    "            if detected_emotion == \"happiness\":\n",
    "                bsay(\"You are happy\")\n",
    "                furhat.gesture(name=\"Smile\")\n",
    "            elif detected_emotion == \"sadness\":\n",
    "                bsay(\"You are sad\")\n",
    "                furhat.gesture(name='Oh')\n",
    "            elif detected_emotion == \"neutral\":\n",
    "                bsay(\"You are neutral\")\n",
    "                furhat.gesture(name='Wink')\n",
    "\n",
    "# Interaction function\n",
    "def interaction():\n",
    "    furhat.set_face(character=FACES['Bartender'], mask=\"Adult\")\n",
    "    furhat.set_voice(name=VOICES_EN['Bartender'])\n",
    "    bsay(\"Hi\")\n",
    "    furhat.gesture(name='BigSmile')\n",
    "\n",
    "    global emotion_detection_active\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cam.read()\n",
    "        if not ret:\n",
    "            print(\"Error: failed to capture image\")\n",
    "            break\n",
    "\n",
<<<<<<< HEAD
    "        detected_emotion = detect_emotion(frame)\n",
    "        speech(recognize_speech())\n",
    "        emotion(detected_emotion)\n",
=======
    "        if emotion_detection_active:\n",
    "            detected_emotion = detect_emotion(frame)\n",
    "            emotion(detected_emotion)\n",
>>>>>>> 007182882facbb7c246a996952c5b04c58f8e5ab
    "\n",
    "        speech_text = recognize_speech()\n",
    "        if speech_text:\n",
    "            print(f\"Recognized: {speech_text}\")\n",
    "            if \"start\" in speech_text.lower():\n",
    "                start_emotion_detection()\n",
    "                bsay(\"Emotion detection started\")\n",
    "            elif \"stop\" in speech_text.lower():\n",
    "                stop_emotion_detection()\n",
    "                bsay(\"Emotion detection stopped\")\n",
    "\n",
    "\n",
    "        \n",
    "        jcv2.imshow(\"Emotion Detection\", frame)\n",
    "        key = jcv2.waitKey(33) & 0xFF\n",
    "        if key == 27:  # ESC pressed\n",
    "            break\n",
    "\n",
    "# Set up camera\n",
    "cam = cv2.VideoCapture(0)\n",
    "cam.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "# main function\n",
    "if __name__ == '__main__':\n",
    "    interaction()\n",
    "\n",
    "cam.release()\n",
    "jcv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "iis",
=======
   "display_name": "Python 3",
>>>>>>> 007182882facbb7c246a996952c5b04c58f8e5ab
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
