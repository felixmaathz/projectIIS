{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from furhat_remote_api import FurhatRemoteAPI\n",
    "import speech_recognition as sr\n",
    "import cv2\n",
    "import opencv_jupyter_ui as jcv2\n",
    "from feat import Detector\n",
    "from feat.utils import FEAT_EMOTION_COLUMNS\n",
    "\n",
    "# Detector choice\n",
    "detector = Detector(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: failed to capture image\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ac08ab638946f39385517b37cd92a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='danger', description='Stop', style=ButtonStyle()), HBox(children=(Label(vaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Furhat IP address\n",
    "FURHAT_IP = \"127.0.1.1\"\n",
    "\n",
    "# Connect to Furhat\n",
    "furhat = FurhatRemoteAPI(FURHAT_IP)\n",
    "furhat.set_led(red=100, green=50, blue=50)\n",
    "\n",
    "# Furhat faces and voices\n",
    "FACES = {'TheJoker': 'Brooklyn'}\n",
    "VOICES_EN = {'TheJoker': 'GregoryNeural'}\n",
    "\n",
    "# Furhat speech\n",
    "def bsay(line):\n",
    "    furhat.say(text=line, blocking=True)\n",
    "    sleep(1) \n",
    "\n",
    "# Speech recognition setup\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Function to recognize speech\n",
    "def recognize_speech():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Say something!\")\n",
    "        audio = recognizer.listen(source, timeout=5)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(\"You said:\", text)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech Recognition could not understand audio.\")\n",
    "        return None\n",
    "\n",
    "# Function for emotion detection\n",
    "def detect_emotion(frame):\n",
    "    faces = detector.detect_faces(frame)\n",
    "    landmarks = detector.detect_landmarks(frame, faces)\n",
    "    emotions = detector.detect_emotions(frame, faces, landmarks)\n",
    "\n",
    "    faces = faces[0]\n",
    "    emotions = emotions[0]\n",
    "\n",
    "    strongest_emotion = emotions.argmax(axis=1)\n",
    "    \n",
    "\n",
    "    for (face, top_emo) in zip(faces, strongest_emotion):\n",
    "        (x0, y0, x1, y1, p) = face\n",
    "        cv2.rectangle(frame, (int(x0), int(y0)), (int(x1), int(y1)), (255, 0, 0), 3)\n",
    "        cv2.putText(frame, FEAT_EMOTION_COLUMNS[top_emo], (int(x0), int(y0 - 10)),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 1.5, (255, 0, 0), 2)\n",
    "\n",
    "        # Return the detected emotion\n",
    "        return FEAT_EMOTION_COLUMNS[top_emo] \n",
    "\n",
    "\n",
    "# Function to react to speech\n",
    "def speech(text):\n",
    "    if text:\n",
    "        if \"water\" in text.lower():\n",
    "            bsay(\"very tasty\")\n",
    "            furhat.gesture(name='Surprise')\n",
    "\n",
    "# Function to react to emotion\n",
    "def emotion(detected_emotion):\n",
    "        \n",
    "            print(f\"Detected emotion: {detected_emotion}\")\n",
    "            if detected_emotion == \"happiness\":\n",
    "                bsay(\"You are happy\")\n",
    "                furhat.gesture(name=\"Smile\")\n",
    "            elif detected_emotion == \"sadness\":\n",
    "                bsay(\"You are sad\")\n",
    "                furhat.gesture(name='Oh')\n",
    "            elif detected_emotion == \"neutral\":\n",
    "                bsay(\"You are neutral\")\n",
    "                furhat.gesture(name='Wink')\n",
    "\n",
    "# Interaction function\n",
    "def interaction():\n",
    "    furhat.set_face(character=FACES['TheJoker'], mask=\"Adult\")\n",
    "    furhat.set_voice(name=VOICES_EN['TheJoker'])\n",
    "    bsay(\"Hi\")\n",
    "    furhat.gesture(name='ExpressDisgust')\n",
    "    while True:\n",
    "        ret, frame = cam.read()\n",
    "        if not ret:\n",
    "            print(\"Error: failed to capture image\")\n",
    "            break\n",
    "\n",
    "        detected_emotion = detect_emotion(frame)\n",
    "        #speech(recognize_speech())\n",
    "        emotion(detected_emotion)\n",
    "\n",
    "        jcv2.imshow(\"Emotion Detection\", frame)\n",
    "        key = jcv2.waitKey(33) & 0xFF\n",
    "        if key == 27:  # ESC pressed\n",
    "            break\n",
    "\n",
    "# Set up camera\n",
    "cam = cv2.VideoCapture(0)\n",
    "cam.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "# main function\n",
    "if __name__ == '__main__':\n",
    "    interaction()\n",
    "\n",
    "cam.release()\n",
    "jcv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Environment",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
