{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from furhat_remote_api import FurhatRemoteAPI\n",
    "import speech_recognition as sr\n",
    "import cv2\n",
    "import opencv_jupyter_ui as jcv2\n",
    "from feat import Detector\n",
    "from feat.utils import FEAT_EMOTION_COLUMNS\n",
    "\n",
    "# Detector choice\n",
    "detector = Detector(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabbe\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected emotion: neutral\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce81905df109447e9017843877f73de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='danger', description='Stop', style=ButtonStyle()), HBox(children=(Label(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2feab1444d41d4b986283aa69d5502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='danger', description='Stop', style=ButtonStyle()), HBox(children=(Label(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cde2982aeb44a28a491254a9f018ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>Emotion Detection</center>'), Canvas()), layout=Layout(border_bottom='1.5px…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Detected emotion: neutral\n",
      "Exection is stopped\n"
     ]
    }
   ],
   "source": [
    "# Furhat IP address\n",
    "FURHAT_IP = \"127.0.1.1\"\n",
    "\n",
    "# Connect to Furhat\n",
    "furhat = FurhatRemoteAPI(FURHAT_IP)\n",
    "furhat.set_led(red=100, green=50, blue=50)\n",
    "\n",
    "# Furhat faces and voices\n",
    "FACES = {'TheJoker': 'Brooklyn'}\n",
    "VOICES_EN = {'TheJoker': 'GregoryNeural'}\n",
    "\n",
    "# Furhat speech\n",
    "def bsay(line):\n",
    "    furhat.say(text=line, blocking=True)\n",
    "    sleep(1) \n",
    "\n",
    "# Speech recognition setup\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Function to recognize speech\n",
    "def recognize_speech():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Say something!\")\n",
    "        audio = recognizer.listen(source, timeout=5)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(\"You said:\", text)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech Recognition could not understand audio.\")\n",
    "        return None\n",
    "\n",
    "# Function for emotion detection\n",
    "def detect_emotion(frame):\n",
    "    faces = detector.detect_faces(frame)\n",
    "    landmarks = detector.detect_landmarks(frame, faces)\n",
    "    emotions = detector.detect_emotions(frame, faces, landmarks)\n",
    "\n",
    "    faces = faces[0]\n",
    "    emotions = emotions[0]\n",
    "\n",
    "    strongest_emotion = emotions.argmax(axis=1)\n",
    "    \n",
    "\n",
    "    for (face, top_emo) in zip(faces, strongest_emotion):\n",
    "        (x0, y0, x1, y1, p) = face\n",
    "        cv2.rectangle(frame, (int(x0), int(y0)), (int(x1), int(y1)), (255, 0, 0), 3)\n",
    "        cv2.putText(frame, FEAT_EMOTION_COLUMNS[top_emo], (int(x0), int(y0 - 10)),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 1.5, (255, 0, 0), 2)\n",
    "\n",
    "        # Return the detected emotion\n",
    "        return FEAT_EMOTION_COLUMNS[top_emo] \n",
    "\n",
    "\n",
    "# Function to react to speech\n",
    "def speech(text):\n",
    "    if text:\n",
    "        if \"water\" in text.lower():\n",
    "            bsay(\"very tasty\")\n",
    "            furhat.gesture(name='Surprise')\n",
    "\n",
    "# Function to react to emotion\n",
    "def emotion(detected_emotion):\n",
    "            print(f\"Detected emotion: {detected_emotion}\")\n",
    "            if detected_emotion == \"happiness\":\n",
    "                bsay(\"You are happy\")\n",
    "                furhat.gesture(name=\"Smile\")\n",
    "            elif detected_emotion == \"sadness\":\n",
    "                bsay(\"You are sad\")\n",
    "                furhat.gesture(name='Oh')\n",
    "            elif detected_emotion == \"neutral\":\n",
    "                bsay(\"You are neutral\")\n",
    "                furhat.gesture(name='Wink')\n",
    "\n",
    "# Interaction function\n",
    "def interaction():\n",
    "    furhat.set_face(character=FACES['TheJoker'], mask=\"Adult\")\n",
    "    furhat.set_voice(name=VOICES_EN['TheJoker'])\n",
    "    bsay(\"Hi\")\n",
    "    furhat.gesture(name='ExpressDisgust')\n",
    "    while True:\n",
    "        ret, frame = cam.read()\n",
    "        if not ret:\n",
    "            print(\"Error: failed to capture image\")\n",
    "            break\n",
    "\n",
    "        detected_emotion = detect_emotion(frame)\n",
    "        speech(recognize_speech())\n",
    "        emotion(detected_emotion)\n",
    "\n",
    "        jcv2.imshow(\"Emotion Detection\", frame)\n",
    "        key = jcv2.waitKey(33) & 0xFF\n",
    "        if key == 27:  # ESC pressed\n",
    "            break\n",
    "\n",
    "# Set up camera\n",
    "cam = cv2.VideoCapture(0)\n",
    "cam.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "# main function\n",
    "if __name__ == '__main__':\n",
    "    interaction()\n",
    "\n",
    "cam.release()\n",
    "jcv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
